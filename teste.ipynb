{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recebeu do servidor: {\"message\": \"Qualquer coisa\"}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "\n",
    "async def test_websocket():\n",
    "    uri = \"ws://localhost:8001/ws/assistant/?api_key=chave_teste\"\n",
    "    async with websockets.connect(uri) as websocket:\n",
    "        message = {\"message\": \"Qualquer coisa\"}\n",
    "        await websocket.send(json.dumps(message))\n",
    "        response = await websocket.recv()\n",
    "        print(f\"Recebeu do servidor: {response}\")\n",
    "\n",
    "await test_websocket()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta ao adicionar: {\"status\": \"item adicionado\"}\n",
      "Itens no carrinho: {\"cart\": [{\"comida__comida\": \"Salad\", \"quantidade\": 1}]}\n",
      "Resposta ao remover: {\"status\": \"item removido\"}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "\n",
    "async def test_cart_consumer():\n",
    "    uri = \"ws://localhost:8001/ws/cart/?token=token_unico\"\n",
    "    async with websockets.connect(uri) as websocket:\n",
    "\n",
    "        comida_id = 4\n",
    "\n",
    "        add_message = {\n",
    "            \"action\": \"add\",\n",
    "            \"comida_id\": comida_id,\n",
    "            \"quantidade\": 2\n",
    "        }\n",
    "        await websocket.send(json.dumps(add_message))\n",
    "        response = await websocket.recv()\n",
    "        print(f\"Resposta ao adicionar: {response}\")\n",
    "\n",
    "        view_message = {\"action\": \"view\"}\n",
    "        await websocket.send(json.dumps(view_message))\n",
    "        response = await websocket.recv()\n",
    "        print(f\"Itens no carrinho: {response}\")\n",
    "\n",
    "        remove_message = {\n",
    "            \"action\": \"remove\",\n",
    "            \"comida_id\": comida_id\n",
    "        }\n",
    "        \n",
    "        await websocket.send(json.dumps(remove_message))\n",
    "        response = await websocket.recv()\n",
    "        print(f\"Resposta ao remover: {response}\")\n",
    "        await websocket.close()\n",
    "\n",
    "await test_cart_consumer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_ZSEWTMUPpMnFcw9UvACcA8kE', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Olá, quais opções de hambuguer estão disponíveis?'), type='text')], created_at=1707315436, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_e1ZoiguG0R0kr910bf70L7je')], object='list', first_id='msg_ZSEWTMUPpMnFcw9UvACcA8kE', last_id='msg_ZSEWTMUPpMnFcw9UvACcA8kE', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "import django\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'core.settings')\n",
    "django.setup()\n",
    "\n",
    "from openai import OpenAI\n",
    "from assistant.env import gpt_key\n",
    "import time\n",
    "\n",
    "class GPT:\n",
    "    def __init__(self) -> None:\n",
    "        self.client = OpenAI(api_key=\"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\")\n",
    "        self.assistant_id = 'asst_UrIDUN3nQaQPvD52nm1WNN3y'\n",
    "        \n",
    "    def retrive_assistant(self):\n",
    "        return self.client.beta.assistants.retrieve(self.assistant_id)\n",
    "    \n",
    "    def list_assistants(self):\n",
    "        return self.client.beta.assistants.list(\n",
    "            order='desc',\n",
    "            limit='20'\n",
    "        )\n",
    "        \n",
    "    def _create_thread(self):\n",
    "        return self.client.beta.threads.create()\n",
    "    \n",
    "    def creat_message(self, message: str, thread_id):\n",
    "        return self.client.beta.threads.messages.create(\n",
    "            thread_id=thread_id,\n",
    "            role='user',\n",
    "            content=message\n",
    "        )\n",
    "    \n",
    "    def run_assistant(self, thread_id, assistant_id):\n",
    "        return self.client.beta.threads.runs.create(\n",
    "            thread_id=thread_id,\n",
    "            assistant_id=assistant_id\n",
    "        )\n",
    "        \n",
    "    def check_status_queue(self, thread_id, run_id):\n",
    "        return self.client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread_id,\n",
    "            run_id=run_id\n",
    "        )\n",
    "\n",
    "    def show_response_assistant(self, message_user):\n",
    "        assistant = self.retrive_assistant()\n",
    "        thread = self._create_thread()\n",
    "        message = self.creat_message(message_user, thread.id)\n",
    "        run = self.run_assistant(thread.id, assistant.id)\n",
    "        \n",
    "        messages = gpt.client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        \n",
    "        return messages\n",
    "\n",
    "\n",
    "gpt = GPT()\n",
    "\n",
    "teste = gpt.show_response_assistant('Olá, quais opções de hambuguer estão disponíveis?')\n",
    "\n",
    "print(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=\"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\")\n",
    "assistant_id = 'asst_UrIDUN3nQaQPvD52nm1WNN3y'\n",
    "\n",
    "my_assistant = client.beta.assistants.retrieve(assistant_id)\n",
    "\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Olá, tem alguma cerveja disponível? reponda apenas com sim ou nao\"\n",
    ")\n",
    "\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=my_assistant.id,\n",
    ")\n",
    "\n",
    "def check_status(thread_id, run_id):\n",
    "    return client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread_id,\n",
    "        run_id=run_id\n",
    "        )\n",
    "\n",
    "status = check_status(thread.id, run.id)\n",
    "\n",
    "while status.status != 'completed':\n",
    "    time.sleep(20)\n",
    "    status = check_status(thread.id, run.id)\n",
    "    print(status.status)\n",
    "    if status.status == 'failed':\n",
    "        print(status)\n",
    "        break\n",
    "\n",
    "print(status.status)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_V7RsVTr96Q1ZwUzA5EQKAVAa', assistant_id='asst_UrIDUN3nQaQPvD52nm1WNN3y', cancelled_at=None, completed_at=1707316812, created_at=1707316808, expires_at=None, failed_at=None, file_ids=['file-uTVXprvLLzcfOykWUS27jS7C', 'file-UPS8gR2JsnKB8OKZEhMRJm9L'], instructions='Olá, tem alguma cerveja disponível? reponda apenas com sim ou nao', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=1707316808, status='completed', thread_id='thread_VOLAYoejGAxGK1EXD3AMoQru', tools=[ToolAssistantToolsCode(type='code_interpreter'), ToolAssistantToolsRetrieval(type='retrieval')], usage=Usage(completion_tokens=129, prompt_tokens=6597, total_tokens=6726))\n",
      "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_L8fhcx7U1WZLVXfkPxu82eN6', assistant_id='asst_UrIDUN3nQaQPvD52nm1WNN3y', content=[MessageContentText(text=Text(annotations=[], value='Sim, há cerveja disponível. O Mr. Hoppy oferece o chope \"Mr. Hoppy Pilsen\" em copos de 300ml por $10 e de 400ml por $12. Além disso, eles também oferecem outras opções de chope e seus respectivos valores, disponíveis para consulta no local【8†fonte】.'), type='text')], created_at=1707316810, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_V7RsVTr96Q1ZwUzA5EQKAVAa', thread_id='thread_VOLAYoejGAxGK1EXD3AMoQru')], object='list', first_id='msg_L8fhcx7U1WZLVXfkPxu82eN6', last_id='msg_L8fhcx7U1WZLVXfkPxu82eN6', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "  thread_id=thread.id\n",
    ")\n",
    "print(status)\n",
    "print(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
